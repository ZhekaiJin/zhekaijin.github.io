<!DOCTYPE HTML>
<html>
	<head>
		<title>Zhekai Jin - Cooper Union</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!-- user-scalable=no -->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- WHERE YOU CAN ADD TRACKING CODE  -->
				<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131804405-1"></script>
				<script>
				  window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'UA-131804405-1');
				</script>
	</head>
	<body class="is-preload">
		<!-- Header -->
			<header id="header">
				<div class="inner">

					<a href="#" class="image avatar"><img src="images/avatar.jpg" alt="" /></a>
					<h1><strong>Zhekai(Scott) Jin</strong><br></h1>
					<h1 style="margin-top: 0.4em"><sup><a href="mailto:jin4@cooper.edu" class="icon fa-envelope">&nbsp;&nbsp;jin4@cooper.edu</a></sup><br></h1>
					<h1><sup><a href="https://www.linkedin.com/in/scott-zhekai-jin-196aa1b1" class="icon fa-linkedin">&nbsp;&nbsp;LinkedIn</a></sup>&nbsp;&nbsp;&nbsp;&nbsp;<sup><a href="https://www.facebook.com/zhekaiJINScott" class="icon fa-facebook">&nbsp;&nbsp;Facebook</a></sup></h1>
					<h1><sup><a href="docs/resume.pdf" class="icon fa-briefcase">&nbsp;&nbsp;Resume</a></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup><a href="https://github.com/zhekaijin" class="icon fa-github">&nbsp;&nbsp;Github</a></sup></h1>
					<h1><sup><a href="docs/cv.pdf" class="icon fa-suitcase">&nbsp;&nbsp;C.V.</a></sup></h1>
				</div>
			</header>

		<!-- Main -->
			<div id="main">
				<!-- One -->
					<section id="one">
						<h2>About</h2>
						<p>I'm a senior-year undergraduate student at <a href="https://cooper.edu/welcome">Cooper Union</a> where I study Electrical and Computer Engineering.
							 My interest lies in Robotics, Perception, Distributed Embedded Systems, Machine Learning and Artificial Intelligence.
						</p>
						<hr/>
						<br>
						<h2>News</h2>
						<ul>
							<li> Working on reasearch topic regarding <a href="http://www.cs.columbia.edu/~ansaf/cing/PTB/index.html">Prediction of Preterm Birth</a> with advanced Machine Learning techniques
								at <a href="https://engfac.cooper.edu//araja/506">Distributed Intelligent Agents Lab (DIAL)</a>
								   with <a href="https://engfac.cooper.edu//araja">Dr. Anita Raja</a>.
							</li>
							<li> Working on reasearch topic regarding Multisensor Data Fusion with Lidar and Stereo Camera, advised by
								<a href="http://faculty.cooper.edu/sable2/">Dr. Carl Sable</a>,
								<a href="https://engfac.cooper.edu/nshlayan">Dr Neveen Shlayan</a>, and
								<a href="https://engfac.cooper.edu/dluchten">Dr. Dirk Luchtenburg</a>
 								, Manuscirpt in preparation</li>
							<li> I joined <a href="https://momenta.ai/en">Momenta.ai</a> as a Lidar Perception research intern working on Object Detection & Tracking, Semantic Segmentation & Ground Detection. </li>
							<li> I joined <a href="http://www.didi-labs.com/">DiDi Lab</a> as an Algorithm Research Intern working on Intelligent Dispatching Solution </li>
							<li> I initiated <a href="https://engfac.cooper.edu/nshlayan/689">Sustainable, Mobile & Agile Connected Communities (SMAC<sup>2</sup>) Lab</a> with <a href="https://cooper.edu/academics/people/neveen-shlayan">Dr. Neveen Shlayan</a> as the first Robotics Lab in Cooper Union.</li>
							</li>
						</ul>

					<hr/>
					<br>
					<h2>Recent Work</h2>
						<!-- 3D reconstruction -->
						<div style="width: 100%; height: 16em">
							<div style="border-radius: 0.35em; float: left; width: 24%; height: 9.5em; margin-right: 1%; background: rgba(45,44,45,1);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/3D_reconstruct.mp4" type="video/mp4">
								</video>
							</div>

							<div style="border-radius: 0.35em; float: left; width: 24%; height: 9.5em; margin-right: 1%; background: rgba(48,48,48,1);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/lidar_mapping.mp4" type="video/mp4">
								</video>
							</div>

							<div style="border-radius: 0.35em; float: left; width: 24%; height: 9.5em; margin-right: 1%; background: rgba(0,43,54,1);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/WiFiTracking.mp4" type="video/mp4">
								</video>
							</div>

							<div style="border-radius: 0.35em; float: left; width: 24%; height: 9.5em; background: rgba(0,43,54,1);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
									<source src="videos/checker.mp4" type="video/mp4">
								</video>
							</div>


							<div style="float: left; width: 24%; margin-right: 1.5%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stereo 3D Reconsturction<br>"How does the robot percieve the world with dynamics and semantics?" <a href="https://p3d.in/x1fMl+load+spin/"> &nbsp;&nbsp;&nbsp;Model Link</a> </div>
							<div style="float: left; width: 24%; margin-right: 1.5%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fast 2D Lidar Mapping<br>"How can robots perform online learning and active perception?" <a href="https://github.com/ZhekaiJin/the-Cooper-Mapper/">Project Link</a> &nbsp;&nbsp;<a href="videos/lidar_mapping.mp4">Video</a></div>
							<div style="float: left; width: 24%; margin-right: 1.5%;font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Real-time WiFi Tracking <br>"How to track people's positions with their phones' WiFi or Bluetooth on?" <a href="http://www.utrc2.org/research/projects/real-time-estimation-transit-origin">Project Link</a> </div>
							<div style="float: left; width: 23%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AI Checker with Minimax<br>"Can you beat a robot player in a zero-sum game like Checker?" <a href="https://github.com/ZhekaiJin/AI-Checker/">Project Link</a> &nbsp;&nbsp;<a href="videos/checker.mp4">Video</a> </div>

						</div>

					<hr>
					<br>
					<h2>Research</h2>
					<p>I work on robotics and autonomous driving. I envision an end-to-end system take rich 3D data and perform efficent 3D scene understanding. I believe the future lies in lifelong learning and long-term SLAM. </p>
					<div class="row">
						<article class="12u 12u$(xsmall) work-item">
							<a href="https://github.com/ZhekaiJin/the-Cooper-Mapper" class="image fit thumb left"><img src="images/projects/car.jpg" alt="" /></a>
							<h3 style="margin-left: 43%">The Cooper Mapper: Self-Driving Robot for MultiSensor Data Fusion investigation</h3>
							<p style="margin-left: 43%">
							To make robots generally useful in the broader world, in applications like autonomous driving, urban search and rescue, AI-assisted precision agriculture, etc., they need to operate in large, three dimensional and potentially unstructured environments.
							Therefore, they need general algorihms for mapping, localizing, planning and exploring that work just about anywhere.
							In this work, we are working toward a general Simultaneous Localization and Mapping (SLAM) solution that fully leverage the advantages of Lidar and Camera sensor,
							has constatn computation time (real-time) and linear in storage space, and utlizes efficient map representation which will be fully 3D and capable of representing arbitrary 3D geometry at a proper level of resolution.
							Our work is aiming to be an extension of <font color="CCCC00">Active SLAM</font> with an efficient multisensor data fusion model.
							<br>
							</p>
							<p style="margin-left: 43%; margin-top: 1em">
							<font color="49bf9d">Zhekai Jin</font>, Simon Shao, Minjoon So<br>
								<a href="https://engfac.cooper.edu/nshlayan/690">Webpage (not up-to-date)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="docs/Semiyear_Report.pdf">Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="https://github.com/ZhekaiJin/the-Cooper-Mapper">Code (Github)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="https://docs.google.com/presentation/d/1sPrlTF6rTZLBEbeD54hqItPvx9GTtTAkng0VJ8XbQQk/edit?usp=sharing">SLAM in 5 MINS</a>
							</p>
						</article>
						<article class="12u 12u$(xsmall) work-item">
							<a href="http://www.utrc2.org/research/projects/real-time-estimation-transit-origin" class="image fit thumb left"><img src="images/projects/iot.png" alt="" /></a>
							<h3 style="margin-left: 43%">The Cooper IoT: Generic Distributed IoT Platform for human traffic monitoring</h3>
							<p style="margin-left: 43%">
							We proposes utilizing Bluetooth/WiFi technology to estimate origin-destination (OD) demands and station wait times of users of any public transportation system.
							If the entrance and exit turnstiles at subway stations are equipped with Bluetooth/WiFi receivers,
							it is possible to capture OD information for some percentage of the riders with visible Bluetooth/WiFi devices.
							The riders who have electronic devices such as most cell phones, iPods, and computers carry unique information in their devicesâ€™ Bluetooth/WiFi MAC address.
							This information can be used scrambled and used anonymously to detect the origin and destination of riders by matching data collected at entrances and exits from the system.
							<!-- This is an on-going study that explores the po-tential benefits of using pedestrian data for evaluation andenhancement of public transportation.
							We proposes the utilization of Bluetooth (BT) and WiFi technologiesto estimate time-dependent origin-destination (OD) demandsand station wait-times of transit bus and subway users.
							We also acquired some preliminary results from multiple pilot field studies, that were conducted at some of the major New YorkCity (NYC) public transportation facilities.
							The main objective of this study is to inquire into the various ways this extensive transit rider data can be used and to establish a general framework through data-driven pedestrian modeling
							within transit stations that renders estimation of key parameters and strategic control of public transportation services possible. -->
							We implemented real-time <font color="427af4">WiFi-and-Bluetooth-addresses-based position tracking</font> and <font color="427af4">human detection</font> algorithms to monitor <font color="CCCC00">human traffic flow</font> and performed <font color="427af4">time-series analysis</font> with a stochastic queuing model.
							<br>
							</p>
							<p style="margin-left: 43%; margin-top: 1em">
							Principal Investigator: Dr. Neveen Shlayan <br>
							<font color="49bf9d">Zhekai Jin</font>, Jialun Bao, Rafi Mueen<br>
								<a href="https://engfac.cooper.edu/nshlayan/691">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="https://ieeexplore.ieee.org/document/7795559">Publication</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="https://trid.trb.org/view/1468647">Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
								<a href="https://github.com/ZhekaiJin/Cooper-IoT">Code (Github)</a>
							</p>
						</article>
					</div>
					<hr/>
					<p style="margin-bottom:0">
					<br>
					<sub>Design: <a href="http://html5up.net">HTML5 UP</a></sub>
					</p>
				</section>
		</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="copyright">
						<li>Meet <a href="https://en.wikipedia.org/wiki/Danbo_(character)">Danbo</a> the cardboard robot.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
